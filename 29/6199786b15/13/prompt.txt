Implement the following plan:

# Plan: Add "About the app" Screen

## Context

The app currently has a single global menu option ("Your Books"). The user wants an "About" screen accessible from this menu, providing app info, author details, and a feedback CTA.

## Changes

### 1. Add translation keys

**File:** `platform/i18n/translations/en.json`

Add `menu.about` and a new `about.*` namespace with all user-facing strings: title, intro paragraph (passion/privacy/open-source), GitHub link label, author section (name, email, copyright, bio), help section (encouragement text, CTA label).

### 2. Add icon fallbacks for non-iOS

**File:** `modules/liquid-glass/src/LiquidGlassButton.tsx`

Add two entries to `SF_SYMBOL_TO_IONICON`:
- `"info.circle"` -> `"information-circle-outline"`
- `"envelope"` -> `"mail-outline"`

### 3. Add "About" menu option

**File:** `features/recipes-list/hooks/useGlobalOptions.ts`

- Append `{ id: "about", label: t("menu.about"), systemImage: "info.circle" }` to the `options` array
- Add `else if (id === "about") { router.push("/about"); }` in `handleSelect`

### 4. Create the About screen

**File (new):** `app/about.tsx`

Layout follows `manage-books.tsx` pattern:
- Full-screen container with `colors.background`
- Header: "About fasola" at `fontSize: 34, fontWeight: "bold"`, `paddingHorizontal: 28`, `paddingTop: insets.top + 16`
- `ScrollView` with `paddingHorizontal: 28`, `paddingBottom: 120` (room for bottom buttons)
- Three sections separated by `marginBottom: 32`:
  - **Intro**: paragraph about passion/privacy/open-source + tappable "View source on GitHub" link (opens via `WebBrowser.openBrowserAsync`)
  - **Author**: name, email, copyright line, bio paragraph
  - **Get help**: encouragement paragraph
- Arrow: unicode arrow character, right-aligned, below the help text, pointing toward the CTA
- `BackButton` component (bottom-left, reused from `lib/components/atoms/BackButton.tsx`)
- CTA `LiquidGlassButton` with `systemImage="envelope"` (bottom-right, same positioning as `EditButton`: `position: "absolute", bottom: 28, right: 28, zIndex: 10`)
- CTA opens `mailto:eryk.napierala@gmail.com?subject=fasola%20feedback` via `Linking.openURL`
- Text colors: `isDark ? "#FFFFFF" : "#000000"` for headings/name, `colors.textSecondary` for body text, `colors.text` for the GitHub link (branded purple in light mode)

## Files Summary

| File | Action |
|---|---|
| `platform/i18n/translations/en.json` | Modify |
| `modules/liquid-glass/src/LiquidGlassButton.tsx` | Modify |
| `features/recipes-list/hooks/useGlobalOptions.ts` | Modify |
| `app/about.tsx` | Create |

## Verification

1. Run `npx tsc --noEmit` to verify types
2. Open the app, tap the ellipsis menu - "About" option should appear with (i) icon
3. Tap "About" - screen renders with header, three sections, arrow, back button, envelope CTA
4. Tap "View source on GitHub" - opens browser to `https://github.com/erykpiast/fasola`
5. Tap envelope CTA - opens email client with prefilled address and subject
6. Tap back button - returns to home screen
7. Test dark mode - text colors adapt correctly


If you need specific details from before exiting plan mode (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/eryk.napierala/.REDACTED.jsonl

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Are all the features actually working, or just pretending to work?

**Code Quality:**
â€¢ Did you leave the code better than you found it?

**Integration & Refactoring:**
â€¢ Would refactoring the surrounding code make everything simpler?

**Codebase Consistency:**
â€¢ Are you following the same patterns used elsewhere in the codebase?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Let's add GitHub icon next to the link to GitHub

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Are there any "Not implemented yet" placeholders or TODO comments in production code?

**Code Quality:**
â€¢ Is every piece of code still serving a clear purpose?

**Integration & Refactoring:**
â€¢ Does the code structure still make sense after your additions?

**Codebase Consistency:**
â€¢ Should other parts of the codebase be updated to match your improvements?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Let's split the "The app is open source" to a separate paragraph with 16 px top margin

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Did you implement the full solution or just the minimum to make tests green?

**Code Quality:**
â€¢ Are you using different patterns than the existing code uses?

**Integration & Refactoring:**
â€¢ Would refactoring the surrounding code make everything simpler?

**Codebase Consistency:**
â€¢ Are you following the same patterns used elsewhere in the codebase?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Let's also separate the on-device sentence

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Does the implementation actually do what it claims, or just return hardcoded values?

**Code Quality:**
â€¢ Is the code more complex now than it needs to be?

**Integration & Refactoring:**
â€¢ Should you extract the new functionality into cleaner abstractions?

**Codebase Consistency:**
â€¢ Are there related files that need the same changes?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Separate each sentence in a helpText key into its own <Text>

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Did you stub out functionality with placeholder messages instead of real logic?

**Code Quality:**
â€¢ Did you clean up after making your changes work?

**Integration & Refactoring:**
â€¢ Would refactoring the surrounding code make everything simpler?

**Codebase Consistency:**
â€¢ Did you update all the places that depend on what you changed?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Attach the arrow to the end of the help text 2

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Did you implement the full solution or just the minimum to make tests green?

**Code Quality:**
â€¢ Did you clean up after making your changes work?

**Integration & Refactoring:**
â€¢ Should you extract the new functionality into cleaner abstractions?

**Codebase Consistency:**
â€¢ Should other parts of the codebase be updated to match your improvements?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Let's make "just drop me an email" a mailto link, too, adn display an envelope icon before that line.

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Are there any "Not implemented yet" placeholders or TODO comments in production code?

**Code Quality:**
â€¢ Is every piece of code still serving a clear purpose?

**Integration & Refactoring:**
â€¢ Should you extract the new functionality into cleaner abstractions?

**Codebase Consistency:**
â€¢ Are you following the same patterns used elsewhere in the codebase?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

# Code Review

## Current Repository State
---
---
19c317f feat: add the About screen Entire-Checkpoint: 296199786b15
0101595 fix: change books management heading
198aa07 fix: add header above the recipes list Entire-Checkpoint: 296199786b15
5085135 feat: align book action buttons with native Apple apps Entire-Checkpoint: 296199786b15
9ecfab8 fix: restore processing indicator Entire-Checkpoint: 296199786b15

## Pre-Review Analysis: Think This Through End-to-End

Before launching review agents, analyze the complete impact and context:

### Impact Assessment
- **System Impact**: What systems, services, or components could be affected by these changes?
- **Deployment Context**: What's the risk level and timeline for these changes?
- **Integration Points**: Are there external dependencies, APIs, or team workflows involved?
- **Stakeholder Impact**: Who depends on the code being reviewed?

### Review Strategy Coordination
Based on impact assessment and ****, determine:
- **Critical vs. Nice-to-Have**: Which review aspects are CRITICAL vs. optional for this change?
- **Potential Conflicts**: Could findings from different review areas suggest competing solutions?
- **Shared Context**: What context should all review agents be aware of?
- **Appropriate Rigor**: What level of analysis matches the change scope and risk?

## Review Strategy

Based on **** and the impact assessment above, determine which review agents are needed:

If reviewing "changes" or recent modifications:
1. Analyze the file types that have been modified
2. Launch only relevant review agents:
   - **Documentation files only** (*.md, *.txt, README): Launch only Documentation & API Review agent
   - **Test files only** (*test.*, *.spec.*, tests/): Launch Testing Quality Review and Code Quality Review agents
   - **Config files only** (*.json, *.yaml, *.toml, .*rc): Launch Security & Dependencies Review and Architecture Review agents
   - **Source code files** (*.ts, *.js, *.py, etc.): Launch all 6 review agents
   - **Mixed changes**: Launch agents relevant to each file type present

If reviewing a specific directory or broad scope:
- Launch all 6 review agents for comprehensive coverage

Use the Task tool to invoke the appropriate code-review-expert agents concurrently with enhanced thinking trigger instructions:

## 1. Architecture & Design Review
```
Subagent: code-review-expert
Description: Architecture review with end-to-end analysis
Prompt: Review the architecture and design patterns in: 

CONTEXT: [Include findings from Pre-Review Analysis above - system impact, deployment context, integration points]

Primary Focus: module organization, separation of concerns, dependency management, abstraction levels, design pattern usage, and architectural consistency. Check available experts with claudekit for domain-specific patterns.

THINK THIS THROUGH END-TO-END:
- Trace architectural impacts: How does this change affect all dependent systems?
- Map the complete data/control flow through the architecture
- Identify what breaks when components fail or change
- Consider the full deployment and integration pipeline
- Analyze how this fits into the broader system architecture

Check available experts with claudekit for domain-specific patterns.
```

## 2. Code Quality Review
```
Subagent: code-review-expert
Description: Code quality review  
Prompt: Review code quality and maintainability in: 
Focus on: readability, naming conventions, code complexity, DRY principles, code smells, refactoring opportunities, and consistent coding patterns. Pull domain-specific quality metrics from available experts.
```

## 3. Security & Dependencies Review
```
Subagent: code-review-expert
Description: Security and dependencies review with alternative hypothesis analysis
Prompt: Perform security and dependency analysis of: 

CONTEXT: [Include findings from Pre-Review Analysis above - system impact, deployment context, integration points]

Primary Focus: input validation, injection vulnerabilities, authentication/authorization, secrets management, dependency vulnerabilities, license compliance, version pinning, and supply chain security. Use security insights from domain experts if available.

CONSIDER ALTERNATIVE HYPOTHESES:
- Beyond obvious vulnerabilities, what other attack vectors exist?
- How else could these security controls be bypassed or exploited?
- What assumptions about user behavior, data flow, or system boundaries could an attacker violate?
- Are there alternative explanations for apparent security measures?
- What if the current security model is fundamentally flawed?

Use security insights from domain experts if available.
```

## 4. Performance & Scalability Review
```
Subagent: code-review-expert
Description: Performance and scalability review
Prompt: Analyze performance and scalability in: 
Focus on: algorithm complexity, memory usage, database queries, caching strategies, async patterns, resource management, load handling, and horizontal scaling considerations. Get performance patterns from relevant experts.
```

## 5. Testing Quality Review
```
Subagent: code-review-expert
Description: Testing quality review
Prompt: Review test quality and effectiveness for: 
Focus on: meaningful assertions, test isolation, edge case handling, failure scenario coverage, mock vs real dependencies balance, test maintainability, clear test names, and actual behavior verification (not just coverage metrics). Check for testing-expert insights if available.
```

## 6. Documentation & API Review
```
Subagent: code-review-expert
Description: Documentation and API review
Prompt: Review documentation and API design for: 

Focus on: README completeness, API documentation, breaking changes, code comments, JSDoc/TypeDoc coverage, usage examples, migration guides, and developer experience. Evaluate API consistency and contract clarity.

Documentation Review Guidelines:
- Consider purpose and audience: Who needs this information and why?
- Evaluate effectiveness: Does the documentation achieve its goals?
- Focus on clarity: Can users understand and apply the information?
- Identify real issues: Missing information, errors, contradictions, outdated content
- Respect intentional variation: Multiple examples may show different valid approaches
```

## Post-Review Consolidation: Consider Alternative Hypotheses

After all agents complete, apply alternative hypothesis thinking before consolidating:

### Cross-Pattern Analysis
- **Competing Solutions**: Do findings from different review areas suggest conflicting solutions or approaches?
- **Alternative Explanations**: Are there alternative explanations for patterns seen across multiple review areas?
- **Root Cause Investigation**: Could the same underlying issue be manifesting in multiple review aspects?
- **Intentional Trade-offs**: What if apparent "problems" are actually intentional design decisions with valid reasoning?

### Prioritization with Context
- **Real vs. Theoretical Issues**: Which issues matter given the actual deployment context and timeline?
- **Conflicting Recommendations**: How do we sequence fixes that might conflict with each other?
- **Alternative Approaches**: If obvious fixes prove problematic, what are the alternative solutions?

Then consolidate findings into this structured format:

```
ğŸ—‚ Consolidated Code Review Report - [Target]

ğŸ“‹ Review Scope
Target: [directory/files reviewed] ([X files, Y lines])
Focus: Architecture, Security, Performance, Testing, Documentation

ğŸ“Š Executive Summary
Brief overview of code quality, key strengths, and critical issues requiring attention.

ğŸ”´ CRITICAL Issues (Must Fix Immediately)
1. ğŸ”’ [Security/ğŸ—ï¸ Architecture/âš¡ Performance/ğŸ§ª Testing/ğŸ“ Documentation/ğŸ’¥ Breaking] [Issue Name]
   File: [path:line]
   Impact: [description]
   Solution:
   ```[code example]```

2. [Additional critical issues with type icons...]

ğŸŸ  HIGH Priority Issues
1. [Type icon] [Issue name]
   File: [path:line]
   Impact: [description]
   Solution: [recommendation]

2. [Additional high priority issues...]

ğŸŸ¡ MEDIUM Priority Issues
1. [Type icon] [Issue name] - [file:line]
   Extract into: [suggested refactoring]

2. [Additional medium priority issues...]

âœ… Quality Metrics
Include only aspects that were actually reviewed based on the file types and agents launched:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect          â”‚ Score â”‚ Notes                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Only include relevant aspects based on what was reviewed]      â”‚
â”‚ Architecture    â”‚ X/10  â”‚ [Clean separation, coupling issues]â”‚
â”‚ Code Quality    â”‚ X/10  â”‚ [Readability, consistency, patterns]â”‚
â”‚ Security        â”‚ X/10  â”‚ [Critical vulnerabilities, if any] â”‚
â”‚ Performance     â”‚ X/10  â”‚ [Bottlenecks, scalability concerns]â”‚
â”‚ Testing         â”‚ X/10  â”‚ [Coverage percentage, test quality]â”‚
â”‚ Documentation   â”‚ X/10  â”‚ [API docs, comments, examples]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For example:
- Documentation-only review: Show only Documentation row
- Test file review: Show Testing and Code Quality rows
- Config file review: Show Security and Architecture rows
- Full code review: Show all relevant aspects

âœ¨ Strengths to Preserve
- [Key strength with evidence]
- [Additional strengths...]

ğŸš€ Proactive Improvements
1. [Pattern/Practice Name]
   ```[code example]```

2. [Additional improvements...]

ğŸ“Š Issue Distribution
- Architecture: [X critical, Y high, Z medium]
- Security: [X critical, Y high, Z medium]
- Performance: [X critical, Y high, Z medium]
- Testing: [X critical, Y high, Z medium]
- Documentation: [X critical, Y high, Z medium]

âš ï¸ Systemic Issues
Repeated problems that need addressing:
- [Problem pattern] (X occurrences)
  â†’ [Actionable fix/next step]
- [Additional problems with solutions...]
```

After all agents complete, consolidate findings into this format. Focus on actionable feedback with specific file locations and code examples. Use type icons:
ğŸ”’ Security | ğŸ—ï¸ Architecture | âš¡ Performance | ğŸ§ª Testing | ğŸ“ Documentation | ğŸ’¥ Breaking Change

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Are there any "Not implemented yet" placeholders or TODO comments in production code?

**Code Quality:**
â€¢ Is the code more complex now than it needs to be?

**Integration & Refactoring:**
â€¢ Does the code structure still make sense after your additions?

**Codebase Consistency:**
â€¢ Should other parts of the codebase be updated to match your improvements?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.

---

Please implement review suggestions

---

Stop hook feedback:
ğŸ“‹ **Self-Review**

Please review these aspects of your changes:

**Implementation Completeness:**
â€¢ Does the implementation actually do what it claims, or just return hardcoded values?

**Code Quality:**
â€¢ Is there duplicated logic that should be extracted?

**Integration & Refactoring:**
â€¢ Should you consolidate similar functions that now exist?

**Codebase Consistency:**
â€¢ Should your solution be applied elsewhere for consistency?

ğŸ’¡ **Tip:** The code-review-expert subagent is available. Use it to review each self-review topic.
Use the Task tool with subagent_type: "code-review-expert"

Address any concerns before proceeding.